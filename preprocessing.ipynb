{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.24 scipy==1.13 smart_open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim==4.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nlpaug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3.9\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import nlpaug.augmenter.word as naw\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords, opinion_lexicon\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced sentiment-aware text cleaning\n",
    "def enhanced_sentiment_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Save emoticons - they contain strong sentiment signals\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Keep hashtags content but remove # symbol\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Handle contractions\n",
    "    text = re.sub(r\"can\\'t\", \"cannot\", text)\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    \n",
    "    # Handle negations with special marking\n",
    "    text = re.sub(r'\\b(?:not|no|never|n\\'t)\\b[\\s]+(\\w+)', r'NOT_\\1', text)\n",
    "    \n",
    "    # Convert to lowercase but preserve elongated words (they indicate emphasis)\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Mark repeated punctuation (often indicates intensity)\n",
    "    text = re.sub(r'([!?.]){2,}', r'\\1 <EMPHASIS>', text)\n",
    "    \n",
    "    # Preserve elongated words as they indicate emphasis\n",
    "    # Then reduce but mark them\n",
    "    text = re.sub(r'(\\w)(\\1{2,})', r'\\1\\1 <ELONGATED>', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Reinsert emoticons\n",
    "    if emoticons:\n",
    "        text += ' ' + ' '.join(emoticons)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Process neutral samples specifically to improve class distinction\n",
    "def process_neutral_samples(text):\n",
    "    \"\"\"Special preprocessing for neutral class samples to enhance their distinction\"\"\"\n",
    "    # Remove intensifiers that might introduce sentiment bias\n",
    "    text = re.sub(r'\\b(very|really|extremely|quite)\\b', '', text)\n",
    "    # Normalize sentiment-laden words\n",
    "    text = re.sub(r'\\b(good|great|excellent|amazing)\\b', 'positive_term', text)\n",
    "    text = re.sub(r'\\b(bad|terrible|awful|horrible)\\b', 'negative_term', text)\n",
    "    return text\n",
    "\n",
    "# Sentiment-preserving augmentation\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def sentiment_preserving_augmentation(text, sentiment_class, max_attempts=5):\n",
    "    \"\"\"Augment text while preserving sentiment\"\"\"\n",
    "    if not text or len(text) < 10:\n",
    "        return text\n",
    "        \n",
    "    # Initialize VADER sentiment analyzer\n",
    "    original_scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # Initialize augmenters\n",
    "    synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    random_aug = naw.RandomWordAug(action=\"swap\")\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Apply augmentation - first synonyms, then word swapping\n",
    "            augmented = synonym_aug.augment(text)\n",
    "            augmented = random_aug.augment(augmented)\n",
    "            \n",
    "            # Check sentiment preservation\n",
    "            new_scores = sia.polarity_scores(augmented)\n",
    "            \n",
    "            # If compound scores are within 0.2, sentiment is likely preserved\n",
    "            if abs(original_scores['compound'] - new_scores['compound']) < 0.2:\n",
    "                return augmented\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # If all attempts fail, return original\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced tokenization for preserving sentiment-relevant text portions\n",
    "def tokenize_with_sentiment_focus(texts, tokenizer, max_length=128):\n",
    "    \"\"\"Tokenize with focus on beginning and end of text (where sentiment often appears)\"\"\"\n",
    "    results = {'input_ids': [], 'attention_mask': []}\n",
    "    \n",
    "    for text in texts:\n",
    "        try:\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            if len(tokens) <= max_length - 2:\n",
    "                encoding = tokenizer(\n",
    "                    text,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    return_tensors='np'\n",
    "                )\n",
    "                results['input_ids'].append(encoding['input_ids'][0])\n",
    "                results['attention_mask'].append(encoding['attention_mask'][0])\n",
    "            else:\n",
    "                # For longer texts\n",
    "                first_chunk = tokens[:max_length//2 - 1]\n",
    "                last_chunk = tokens[-(max_length//2 - 1):]\n",
    "                \n",
    "                # Create combined tokens with special tokens\n",
    "                combined_tokens = [tokenizer.cls_token] + first_chunk + [tokenizer.sep_token] + last_chunk + [tokenizer.sep_token]\n",
    "                combined_ids = tokenizer.convert_tokens_to_ids(combined_tokens)\n",
    "                attention_mask = [1] * len(combined_ids)\n",
    "                \n",
    "                # Pad to max_length\n",
    "                pad_length = max_length - len(combined_ids)\n",
    "                combined_ids = combined_ids + [tokenizer.pad_token_id] * pad_length\n",
    "                attention_mask = attention_mask + [0] * pad_length\n",
    "                \n",
    "                # Truncate\n",
    "                combined_ids = combined_ids[:max_length]\n",
    "                attention_mask = attention_mask[:max_length]\n",
    "                \n",
    "                results['input_ids'].append(np.array(combined_ids))\n",
    "                results['attention_mask'].append(np.array(attention_mask))\n",
    "        except Exception as e:\n",
    "            # Fallback for errors\n",
    "            print(f\"Error tokenizing text: {e}\")\n",
    "            # Use empty padding\n",
    "            empty_input_ids = np.array([tokenizer.cls_token_id] + [tokenizer.pad_token_id] * (max_length - 2) + [tokenizer.sep_token_id])\n",
    "            empty_attention_mask = np.array([1] + [0] * (max_length - 2) + [1])\n",
    "            results['input_ids'].append(empty_input_ids)\n",
    "            results['attention_mask'].append(empty_attention_mask)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(results['input_ids']),\n",
    "        'attention_mask': np.array(results['attention_mask'])\n",
    "    }\n",
    "\n",
    "# LDA feature extraction with better convergence\n",
    "def extract_lda_features(train_texts, val_texts, test_texts, n_topics=15, random_state=42):\n",
    "    \"\"\"Extract LDA topic distributions from text data with improved convergence\"\"\"\n",
    "    print(f\"\\nExtracting LDA topic features (n_topics={n_topics})...\")\n",
    "    print(\"Creating document-term matrix...\")\n",
    "    \n",
    "    # Get sentiment words\n",
    "    try:\n",
    "        positive_words = set(opinion_lexicon.positive())\n",
    "        negative_words = set(opinion_lexicon.negative())\n",
    "        sentiment_words = positive_words.union(negative_words)\n",
    "        print(f\"Loaded {len(sentiment_words)} sentiment words from lexicon\")\n",
    "    except:\n",
    "        sentiment_words = set()\n",
    "        print(\"Warning: Could not load sentiment lexicon\")\n",
    "    \n",
    "    # Create vectorizer with expanded vocabulary\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        max_df=0.95, \n",
    "        min_df=2, \n",
    "        stop_words='english', \n",
    "        max_features=2500\n",
    "    )\n",
    "    \n",
    "    train_dtm = count_vectorizer.fit_transform(train_texts)\n",
    "    \n",
    "    # Transform validation and test texts\n",
    "    val_dtm = count_vectorizer.transform(val_texts)\n",
    "    test_dtm = count_vectorizer.transform(test_texts)\n",
    "    \n",
    "    # Fit LDA model on training data only with improved settings\n",
    "    print(\"Fitting LDA model...\")\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=random_state,\n",
    "        max_iter=50,  \n",
    "        n_jobs=-1,    \n",
    "        evaluate_every=5,\n",
    "        learning_method='online',\n",
    "        learning_offset=50.0,\n",
    "        batch_size=128,\n",
    "        verbose=1  \n",
    "    )\n",
    "    lda_model.fit(train_dtm)\n",
    "    \n",
    "    # Transform data to get topic distributions\n",
    "    print(\"Extracting topic distributions...\")\n",
    "    train_topic_dists = lda_model.transform(train_dtm)\n",
    "    val_topic_dists = lda_model.transform(val_dtm)\n",
    "    test_topic_dists = lda_model.transform(test_dtm)\n",
    "    \n",
    "    # Print top words for each topic\n",
    "    feature_names = count_vectorizer.get_feature_names_out()\n",
    "    print(\"\\nTop 10 words for each topic:\")\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]  \n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        print(f\"Topic #{topic_idx}: {', '.join(top_words)}\")\n",
    "    \n",
    "    # Print log-likelihood to evaluate convergence\n",
    "    print(f\"Final log-likelihood: {lda_model.score(train_dtm)}\")\n",
    "    \n",
    "    # Save LDA model and vectorizer for later use\n",
    "    with open('lda_model.pkl', 'wb') as f:\n",
    "        pickle.dump(lda_model, f)\n",
    "    with open('count_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(count_vectorizer, f)\n",
    "        \n",
    "    print(f\"LDA extraction complete. Feature shape: {train_topic_dists.shape[1]} topics per document\")\n",
    "    \n",
    "    return train_topic_dists, val_topic_dists, test_topic_dists, lda_model, count_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define data directories using proper path handling\n",
    "    DATA_DIR = 'raw data'\n",
    "    print(\"Loading datasets...\")\n",
    "    try:\n",
    "        # Load Twitter dataset\n",
    "        twitter_train = pd.read_csv(os.path.join(DATA_DIR, 'twitter_training.csv'), header=None, \n",
    "                                   names=['Tweet id','topic', 'sentiment','Tweet content'])\n",
    "        twitter_val = pd.read_csv(os.path.join(DATA_DIR, 'twitter_validation.csv'), header=None, \n",
    "                                 names=['Tweet id','topic', 'sentiment','Tweet content'])\n",
    "        twitter = pd.concat([twitter_train, twitter_val], ignore_index=True)\n",
    "        \n",
    "        # Load climate dataset\n",
    "        climate_text = pd.read_csv(os.path.join(DATA_DIR, 'twitter_sentiment_data.csv'))\n",
    "        \n",
    "        # Load YouTube dataset\n",
    "        youtube_comments = pd.read_csv(os.path.join(DATA_DIR, 'YoutubeCommentsDataSet.csv'))\n",
    "\n",
    "        # Drop irrelevant rows from Twitter\n",
    "        twitter = twitter[twitter['sentiment'] != 'Irrelevant']\n",
    "        \n",
    "        # Map YouTube labels to standardized format\n",
    "        capital = {\n",
    "            'positive': 'Positive',\n",
    "            'negative': 'Negative',\n",
    "            'neutral': 'Neutral'\n",
    "        }\n",
    "        youtube_comments['Sentiment'] = youtube_comments['Sentiment'].map(capital)\n",
    "        \n",
    "        sentiment_map = {\n",
    "            -1: 'Negative',\n",
    "            0: 'Neutral',\n",
    "            1: 'Positive'\n",
    "        }\n",
    "        climate_text['sentiment'] = climate_text['sentiment'].map(sentiment_map)\n",
    "\n",
    "        # Remove problem entries\n",
    "        climate_text = climate_text[climate_text['sentiment'] != 2]\n",
    "        \n",
    "        print(\"Applying enhanced sentiment-aware text cleaning...\")\n",
    "        # Apply enhanced preprocessing to better maintain sentiment markers\n",
    "        twitter['text'] = twitter['Tweet content'].apply(enhanced_sentiment_clean)\n",
    "        youtube_comments['text'] = youtube_comments['Comment'].apply(enhanced_sentiment_clean)\n",
    "        climate_text['text'] = climate_text['message'].apply(enhanced_sentiment_clean)\n",
    "        \n",
    "        # Prepare subsets with standardized column names\n",
    "        twitter_subset = twitter[['text', 'sentiment']]\n",
    "        youtube_subset = youtube_comments[['text', 'Sentiment']].rename(columns={'Sentiment': 'sentiment'})\n",
    "        climate_subset = climate_text[['text', 'sentiment']]\n",
    "        \n",
    "        # Combine all datasets\n",
    "        print(\"Concatenating datasets...\")\n",
    "        combined_data = pd.concat([twitter_subset, youtube_subset, climate_subset], ignore_index=True)\n",
    "\n",
    "        # Remove empty texts and duplicates\n",
    "        combined_data = combined_data[combined_data['text'].str.len() > 5].drop_duplicates()\n",
    "\n",
    "        # ADDED: Identify and drop rows with NaN sentiment values\n",
    "        nan_sentiment = combined_data[combined_data['sentiment'].isna()]\n",
    "        print(f\"Found {len(nan_sentiment)} rows with NaN sentiment values\")\n",
    "        if len(nan_sentiment) > 0:\n",
    "            print(\"Sample rows with NaN sentiment:\")\n",
    "            print(nan_sentiment.head())\n",
    "\n",
    "        # Drop rows with NaN sentiment\n",
    "        combined_data = combined_data.dropna(subset=['sentiment'])\n",
    "        print(f\"After dropping NaNs: {len(combined_data)} rows remain\")\n",
    "\n",
    "        # Apply class-specific preprocessing for neutral class\n",
    "        print(\"Applying class-specific preprocessing for neutral class...\")\n",
    "        neutral_mask = combined_data['sentiment'] == 'Neutral'\n",
    "        combined_data.loc[neutral_mask, 'text'] = combined_data.loc[neutral_mask, 'text'].apply(process_neutral_samples)\n",
    "        \n",
    "        # Encode sentiment labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        combined_data['sentiment_encoded'] = label_encoder.fit_transform(combined_data['sentiment'])\n",
    "        sentiment_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "        print(\"Sentiment mapping:\", sentiment_mapping)\n",
    "        \n",
    "        # Analyze class distribution\n",
    "        class_distribution = combined_data['sentiment_encoded'].value_counts()\n",
    "        print(\"\\nClass distribution before handling imbalance:\")\n",
    "        print(class_distribution)\n",
    "        print(f\"Class ratios: {class_distribution / len(combined_data)}\")\n",
    "        \n",
    "        # Split data into train/val/test sets (before augmentation to prevent data leakage)\n",
    "        print(\"\\nCreating data splits...\")\n",
    "        train_data, test_data = train_test_split(\n",
    "            combined_data, test_size=0.2, random_state=42, \n",
    "            stratify=combined_data['sentiment_encoded']\n",
    "        )\n",
    "        train_data, val_data = train_test_split(\n",
    "            train_data, test_size=0.25, random_state=42, \n",
    "            stratify=train_data['sentiment_encoded']\n",
    "        )\n",
    "        \n",
    "        print(f\"Original train set: {len(train_data)} samples\")\n",
    "        print(f\"Validation set: {len(val_data)} samples\")\n",
    "        print(f\"Test set: {len(test_data)} samples\")\n",
    "        \n",
    "        # Extract LDA features at multiple granularities\n",
    "        print(\"\\nExtracting LDA features at multiple granularities...\")\n",
    "        lda_topic_ranges = [15, 25]  \n",
    "        \n",
    "        # Extract features for the default number of topics\n",
    "        lda_n_topics = 15  # Default number of topics\n",
    "        train_topic_dists, val_topic_dists, test_topic_dists, lda_model, count_vectorizer = extract_lda_features(\n",
    "            train_data['text'].tolist(), \n",
    "            val_data['text'].tolist(), \n",
    "            test_data['text'].tolist(), \n",
    "            n_topics=lda_n_topics\n",
    "        )\n",
    "        \n",
    "        # Store additional LDA features with different topic counts\n",
    "        for n_topics in lda_topic_ranges:\n",
    "            if n_topics != lda_n_topics:  \n",
    "                print(f\"\\nExtracting LDA features with {n_topics} topics...\")\n",
    "                train_topics_alt, val_topics_alt, test_topics_alt, _, _ = extract_lda_features(\n",
    "                    train_data['text'].tolist(), \n",
    "                    val_data['text'].tolist(), \n",
    "                    test_data['text'].tolist(), \n",
    "                    n_topics=n_topics\n",
    "                )\n",
    "                # Save these additional features\n",
    "                np.save(f'train_lda_topics_{n_topics}.npy', train_topics_alt)\n",
    "                np.save(f'val_lda_topics_{n_topics}.npy', val_topics_alt)\n",
    "                np.save(f'test_lda_topics_{n_topics}.npy', test_topics_alt)\n",
    "        \n",
    "        # Add LDA topic distributions as features to each dataset\n",
    "        for i in range(lda_n_topics):\n",
    "            train_data[f'topic_{i}'] = train_topic_dists[:, i]\n",
    "            val_data[f'topic_{i}'] = val_topic_dists[:, i]\n",
    "            test_data[f'topic_{i}'] = test_topic_dists[:, i]\n",
    "        \n",
    "        # Save topic distributions separately\n",
    "        np.save('train_lda_topics.npy', train_topic_dists)\n",
    "        np.save('val_lda_topics.npy', val_topic_dists)\n",
    "        np.save('test_lda_topics.npy', test_topic_dists)\n",
    "        \n",
    "        # Handle class imbalance\n",
    "        print(\"\\nHandling class imbalance...\")\n",
    "        \n",
    "        # 1. Calculate class distribution in training data\n",
    "        train_class_dist = train_data['sentiment_encoded'].value_counts()\n",
    "        print(\"Train set class distribution before balancing:\")\n",
    "        print(train_class_dist)\n",
    "        \n",
    "        majority_class = train_class_dist.idxmax()\n",
    "        minority_classes = [cls for cls in train_class_dist.index if cls != majority_class]\n",
    "        \n",
    "        # 3. Apply sentiment-preserving text augmentation for minority classes\n",
    "        augmented_rows = []\n",
    "        \n",
    "        print(\"Applying sentiment-preserving text augmentation for minority classes...\")\n",
    "        for minority_class in minority_classes:\n",
    "            # Get samples from minority class\n",
    "            minority_samples = train_data[train_data['sentiment_encoded'] == minority_class]\n",
    "            \n",
    "            # Calculate how many more samples needed to reach balanced ratio (80% of majority)\n",
    "            target_count = int(train_class_dist[majority_class] * 0.8)  \n",
    "            samples_needed = min(target_count - len(minority_samples), len(minority_samples))\n",
    "            samples_needed = max(0, samples_needed) \n",
    "            \n",
    "            if samples_needed > 0:\n",
    "                print(f\"Class {minority_class}: Generating {samples_needed} augmented samples\")\n",
    "                \n",
    "                # Random sampling with replacement if we need more samples than available\n",
    "                if samples_needed > len(minority_samples):\n",
    "                    # Multiple rounds of augmentation may be needed\n",
    "                    augmentation_rounds = (samples_needed // len(minority_samples)) + 1\n",
    "                    \n",
    "                    for round_idx in range(augmentation_rounds):\n",
    "                        # Process all minority samples in this round\n",
    "                        for idx, row in minority_samples.iterrows():\n",
    "                            # Skip if we've generated enough samples\n",
    "                            if len(augmented_rows) >= samples_needed:\n",
    "                                break\n",
    "                                \n",
    "                            original_text = row['text']\n",
    "                            sentiment_class = row['sentiment']\n",
    "                            \n",
    "                            # Apply sentiment-preserving augmentation\n",
    "                            augmented_text = sentiment_preserving_augmentation(\n",
    "                                original_text, \n",
    "                                sentiment_class\n",
    "                            )\n",
    "                            \n",
    "                            # Skip if augmentation didn't change the text\n",
    "                            if augmented_text == original_text:\n",
    "                                continue\n",
    "                            \n",
    "                            # Create new row with augmented text\n",
    "                            new_row = row.copy()\n",
    "                            new_row['text'] = augmented_text\n",
    "                            \n",
    "                            # Generate new LDA topics for augmented text\n",
    "                            # We need to do this to ensure LDA features are consistent\n",
    "                            augmented_dtm = count_vectorizer.transform([augmented_text])\n",
    "                            augmented_topics = lda_model.transform(augmented_dtm)[0]\n",
    "                            \n",
    "                            # Add topic distributions to the new row\n",
    "                            for topic_idx in range(lda_n_topics):\n",
    "                                new_row[f'topic_{topic_idx}'] = augmented_topics[topic_idx]\n",
    "                                \n",
    "                            augmented_rows.append(new_row)\n",
    "                else:\n",
    "                    # Randomly select samples to augment\n",
    "                    samples_to_augment = minority_samples.sample(n=samples_needed, random_state=42)\n",
    "                    \n",
    "                    for idx, row in samples_to_augment.iterrows():\n",
    "                        original_text = row['text']\n",
    "                        sentiment_class = row['sentiment']\n",
    "                        \n",
    "                        # Apply sentiment-preserving augmentation\n",
    "                        augmented_text = sentiment_preserving_augmentation(\n",
    "                            original_text, \n",
    "                            sentiment_class\n",
    "                        )\n",
    "                        \n",
    "                        # Skip if augmentation didn't change the text\n",
    "                        if augmented_text == original_text:\n",
    "                            continue\n",
    "                        \n",
    "                        # Create new row with augmented text\n",
    "                        new_row = row.copy()\n",
    "                        new_row['text'] = augmented_text\n",
    "                        \n",
    "                        # Generate new LDA topics for augmented text\n",
    "                        augmented_dtm = count_vectorizer.transform([augmented_text])\n",
    "                        augmented_topics = lda_model.transform(augmented_dtm)[0]\n",
    "                        \n",
    "                        # Add topic distributions to the new row\n",
    "                        for topic_idx in range(lda_n_topics):\n",
    "                            new_row[f'topic_{topic_idx}'] = augmented_topics[topic_idx]\n",
    "                            \n",
    "                        augmented_rows.append(new_row)\n",
    "        \n",
    "        # Create DataFrame with augmented samples and concatenate with original training data\n",
    "        if augmented_rows:\n",
    "            augmented_df = pd.DataFrame(augmented_rows)\n",
    "            augmented_train_data = pd.concat([train_data, augmented_df], ignore_index=True)\n",
    "            print(f\"After text augmentation, train set size: {len(augmented_train_data)} samples\")\n",
    "            \n",
    "            # Check class distribution after text augmentation\n",
    "            print(\"Class distribution after text augmentation:\")\n",
    "            print(augmented_train_data['sentiment_encoded'].value_counts())\n",
    "        else:\n",
    "            augmented_train_data = train_data.copy()\n",
    "        \n",
    "        # Apply SMOTE to topic distributions, not raw text\n",
    "        print(\"\\nApplying SMOTE to topic distributions for better coherence...\")\n",
    "        topic_columns = [col for col in augmented_train_data.columns if col.startswith('topic_')]\n",
    "        X_topics = augmented_train_data[topic_columns].values\n",
    "        y = augmented_train_data['sentiment_encoded'].values\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_topics, y)\n",
    "        \n",
    "        # Get original samples - SMOTE preserves them at the beginning\n",
    "        original_count = len(augmented_train_data)\n",
    "        original_indices = list(range(min(original_count, X_resampled.shape[0])))\n",
    "        \n",
    "        # Create a new DataFrame with the original samples\n",
    "        smote_train_data = augmented_train_data.iloc[original_indices].copy()\n",
    "        \n",
    "        # Create synthetic samples and update with SMOTE topic distributions\n",
    "        synthetic_count = len(y_resampled) - len(original_indices)\n",
    "        \n",
    "        if synthetic_count > 0:\n",
    "            print(f\"Creating {synthetic_count} synthetic samples with SMOTE topic distributions\")\n",
    "            synthetic_rows = []\n",
    "            \n",
    "            # For each synthetic sample needed\n",
    "            for outer_idx in range(synthetic_count):\n",
    "                # Find the class of this synthetic sample\n",
    "                synthetic_class = y_resampled[len(original_indices) + outer_idx]\n",
    "                synthetic_topics = X_resampled[len(original_indices) + outer_idx]\n",
    "                \n",
    "                # Find a random real sample of this class\n",
    "                real_samples_of_class = augmented_train_data[augmented_train_data['sentiment_encoded'] == synthetic_class]\n",
    "                \n",
    "                if len(real_samples_of_class) > 0:\n",
    "                    # Select a random sample\n",
    "                    sample_idx = real_samples_of_class.sample(1).index[0]\n",
    "                    sample_row = augmented_train_data.loc[sample_idx]\n",
    "                    \n",
    "                    # Apply sentiment-preserving augmentation\n",
    "                    original_text = sample_row['text']\n",
    "                    sentiment_class = sample_row['sentiment']\n",
    "                    augmented_text = sentiment_preserving_augmentation(original_text, sentiment_class)\n",
    "                    \n",
    "                    # Create new row\n",
    "                    new_row = sample_row.copy()\n",
    "                    new_row['text'] = augmented_text\n",
    "                    \n",
    "                    # Add the SMOTE-generated topic distributions\n",
    "                    for i, col in enumerate(topic_columns):\n",
    "                        new_row[col] = synthetic_topics[i]\n",
    "                        \n",
    "                    synthetic_rows.append(new_row)\n",
    "            \n",
    "            # Create DataFrame with synthetic samples\n",
    "            if synthetic_rows:\n",
    "                synthetic_df = pd.DataFrame(synthetic_rows)\n",
    "                # Combine with sampled originals\n",
    "                smote_train_data = pd.concat([smote_train_data, synthetic_df], ignore_index=True)\n",
    "            \n",
    "        # Final training data is the SMOTE-balanced dataset\n",
    "        train_data = smote_train_data\n",
    "        print(f\"Final balanced train set size: {len(train_data)} samples\")\n",
    "        \n",
    "        # 4. Calculate final class distribution\n",
    "        final_class_dist = train_data['sentiment_encoded'].value_counts()\n",
    "        print(\"\\nFinal class distribution after balancing:\")\n",
    "        print(final_class_dist)\n",
    "        print(f\"Final class ratios: {final_class_dist / len(train_data)}\")\n",
    "        \n",
    "        # 5. Calculate class weights for model training\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced', \n",
    "            classes=np.unique(train_data['sentiment_encoded']), \n",
    "            y=train_data['sentiment_encoded']\n",
    "        )\n",
    "        class_weight_dict = dict(zip(np.unique(train_data['sentiment_encoded']), class_weights))\n",
    "        print(\"\\nComputed class weights:\", class_weight_dict)\n",
    "        \n",
    "        # Save class weights for model training\n",
    "        with open('class_weights.pkl', 'wb') as f:\n",
    "            pickle.dump(class_weight_dict, f)\n",
    "        \n",
    "        # Save datasets\n",
    "        print(\"Saving preprocessed data...\")\n",
    "        train_data.to_csv('train_data_balanced.csv', index=False)\n",
    "        val_data.to_csv('val_data.csv', index=False)\n",
    "        test_data.to_csv('test_data.csv', index=False)\n",
    "        \n",
    "        # Pre-tokenize using Albert with enhanced tokenization\n",
    "        print(\"Pre-tokenizing data with sentiment-aware tokenization...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "        \n",
    "        for name, dataset in [(\"train\", train_data), (\"val\", val_data), (\"test\", test_data)]:\n",
    "            print(f\"Pre-tokenizing {name} set with sentiment focus...\")\n",
    "            \n",
    "            # Use enhanced tokenization that preserves beginning and end portions\n",
    "            encodings = tokenize_with_sentiment_focus(\n",
    "                dataset['text'].tolist(),\n",
    "                tokenizer,\n",
    "                max_length=128\n",
    "            )\n",
    "            \n",
    "            # Save tokenized data as numpy arrays\n",
    "            np.save(f'{name}_input_ids.npy', encodings['input_ids'])\n",
    "            np.save(f'{name}_attention_mask.npy', encodings['attention_mask'])\n",
    "            np.save(f'{name}_labels.npy', dataset['sentiment_encoded'].values)\n",
    "            \n",
    "            # Also save LDA features separately for each split\n",
    "            topic_columns = [col for col in dataset.columns if col.startswith('topic_')]\n",
    "            if topic_columns:\n",
    "                np.save(f'{name}_lda_features.npy', dataset[topic_columns].values)\n",
    "        \n",
    "        # Save label encoder for inference\n",
    "        with open('sentiment_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump(label_encoder, f)\n",
    "            \n",
    "        print(\"Preprocessing complete with enhanced sentiment preservation, LDA feature extraction, and class balancing!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
