{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:08:48.659902Z",
     "iopub.status.busy": "2025-04-25T01:08:48.659133Z",
     "iopub.status.idle": "2025-04-25T01:08:52.419876Z",
     "shell.execute_reply": "2025-04-25T01:08:52.419051Z",
     "shell.execute_reply.started": "2025-04-25T01:08:48.659875Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade bigframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:08:52.421549Z",
     "iopub.status.busy": "2025-04-25T01:08:52.421279Z",
     "iopub.status.idle": "2025-04-25T01:08:55.344477Z",
     "shell.execute_reply": "2025-04-25T01:08:55.343550Z",
     "shell.execute_reply.started": "2025-04-25T01:08:52.421526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install toolz==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:08:55.345975Z",
     "iopub.status.busy": "2025-04-25T01:08:55.345655Z",
     "iopub.status.idle": "2025-04-25T01:08:56.458288Z",
     "shell.execute_reply": "2025-04-25T01:08:56.457391Z",
     "shell.execute_reply.started": "2025-04-25T01:08:55.345940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip uninstall rich --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:08:56.460664Z",
     "iopub.status.busy": "2025-04-25T01:08:56.460413Z",
     "iopub.status.idle": "2025-04-25T01:08:59.833254Z",
     "shell.execute_reply": "2025-04-25T01:08:59.832538Z",
     "shell.execute_reply.started": "2025-04-25T01:08:56.460642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install rich==13.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:08:59.834651Z",
     "iopub.status.busy": "2025-04-25T01:08:59.834359Z",
     "iopub.status.idle": "2025-04-25T01:09:03.089983Z",
     "shell.execute_reply": "2025-04-25T01:09:03.089247Z",
     "shell.execute_reply.started": "2025-04-25T01:08:59.834628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:03.091196Z",
     "iopub.status.busy": "2025-04-25T01:09:03.090954Z",
     "iopub.status.idle": "2025-04-25T01:09:04.323669Z",
     "shell.execute_reply": "2025-04-25T01:09:04.322779Z",
     "shell.execute_reply.started": "2025-04-25T01:09:03.091174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip uninstall scikit-learn --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:04.325130Z",
     "iopub.status.busy": "2025-04-25T01:09:04.324808Z",
     "iopub.status.idle": "2025-04-25T01:09:09.764371Z",
     "shell.execute_reply": "2025-04-25T01:09:09.763380Z",
     "shell.execute_reply.started": "2025-04-25T01:09:04.325097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:09.765764Z",
     "iopub.status.busy": "2025-04-25T01:09:09.765509Z",
     "iopub.status.idle": "2025-04-25T01:09:12.784538Z",
     "shell.execute_reply": "2025-04-25T01:09:12.783500Z",
     "shell.execute_reply.started": "2025-04-25T01:09:09.765740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:12.786186Z",
     "iopub.status.busy": "2025-04-25T01:09:12.785868Z",
     "iopub.status.idle": "2025-04-25T01:09:16.315271Z",
     "shell.execute_reply": "2025-04-25T01:09:16.314325Z",
     "shell.execute_reply.started": "2025-04-25T01:09:12.786153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:16.318649Z",
     "iopub.status.busy": "2025-04-25T01:09:16.318156Z",
     "iopub.status.idle": "2025-04-25T01:09:21.330217Z",
     "shell.execute_reply": "2025-04-25T01:09:21.329172Z",
     "shell.execute_reply.started": "2025-04-25T01:09:16.318613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade grpcio-status tensorflow-metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:21.331751Z",
     "iopub.status.busy": "2025-04-25T01:09:21.331434Z",
     "iopub.status.idle": "2025-04-25T01:09:26.728934Z",
     "shell.execute_reply": "2025-04-25T01:09:26.728116Z",
     "shell.execute_reply.started": "2025-04-25T01:09:21.331716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== NLTK Resource Setup =====\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "def download_nltk_resources():\n",
    "    \"\"\"Download all required NLTK resources if not already present.\"\"\"\n",
    "    resources = ['punkt', 'wordnet', 'vader_lexicon', 'opinion_lexicon', 'averaged_perceptron_tagger']\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.data.find(resource)\n",
    "            print(f\"{resource} already downloaded\")\n",
    "        except LookupError:\n",
    "            print(f\"Downloading {resource}...\")\n",
    "            nltk.download(resource)\n",
    "\n",
    "download_nltk_resources()\n",
    "\n",
    "# ===== Imports & Configuration =====\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchvision.ops import StochasticDepth\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from functools import partial\n",
    "from torch.amp import autocast, GradScaler\n",
    "import json\n",
    "from huggingface_hub import login, hf_hub_download, upload_file\n",
    "\n",
    "\n",
    "# Configuration parameters\n",
    "BASE_PATH = '/kaggle/input/optimized-set'\n",
    "MODEL_SAVE_PATH = './hybrid_sentiment_model.pt'\n",
    "BERT_MODEL_NAME = 'albert-base-v2'\n",
    "NUM_CLASSES = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 3e-05\n",
    "HIDDEN_SIZE = 768\n",
    "LDA_TOPICS = 25\n",
    "USE_MIXED_PRECISION = True\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===== Load Serialized Objects =====\n",
    "try:\n",
    "    with open(os.path.join(BASE_PATH, 'count_vectorizer.pkl'), 'rb') as f:\n",
    "        count_vectorizer = pickle.load(f)\n",
    "    with open(os.path.join(BASE_PATH, 'lda_model.pkl'), 'rb') as f:\n",
    "        lda_model = pickle.load(f)\n",
    "    with open(os.path.join(BASE_PATH, 'class_weights.pkl'), 'rb') as f:\n",
    "        class_weights = pickle.load(f)\n",
    "    with open(os.path.join(BASE_PATH, 'sentiment_encoder.pkl'), 'rb') as f:\n",
    "        sentiment_encoder = pickle.load(f)\n",
    "\n",
    "    # Optionally extract values for use later on\n",
    "    VOCAB_SIZE = len(count_vectorizer.vocabulary_)\n",
    "    NUM_TOPICS = lda_model.n_components\n",
    "    NUM_CLASSES = len(sentiment_encoder.classes_)\n",
    "\n",
    "    print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "    print(f\"Number of LDA topics: {NUM_TOPICS}\")\n",
    "    print(f\"Number of sentiment classes: {NUM_CLASSES}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading serialized objects: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:26.730307Z",
     "iopub.status.busy": "2025-04-25T01:09:26.729877Z",
     "iopub.status.idle": "2025-04-25T01:09:26.747690Z",
     "shell.execute_reply": "2025-04-25T01:09:26.746884Z",
     "shell.execute_reply.started": "2025-04-25T01:09:26.730282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Dataset & Data Loading =====\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class that loads pre-tokenized inputs along with\n",
    "    multiple LDA topic distributions and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_ids, attention_mask, topic_dist, topic_dist_25=None, labels=None):\n",
    "        # Find the minimum length across all inputs\n",
    "        min_len = min(len(input_ids), len(attention_mask), len(topic_dist))\n",
    "        if labels is not None:\n",
    "            min_len = min(min_len, len(labels))\n",
    "        if topic_dist_25 is not None:\n",
    "            min_len = min(min_len, len(topic_dist_25))\n",
    "            \n",
    "        # Truncate all inputs to the minimum length\n",
    "        self.input_ids = torch.tensor(input_ids[:min_len], dtype=torch.long)\n",
    "        self.attention_mask = torch.tensor(attention_mask[:min_len], dtype=torch.long)\n",
    "        self.topic_dist = torch.tensor(topic_dist[:min_len], dtype=torch.float)\n",
    "        self.has_multi_granularity = topic_dist_25 is not None\n",
    "        \n",
    "        if self.has_multi_granularity:\n",
    "            self.topic_dist_25 = torch.tensor(topic_dist_25[:min_len], dtype=torch.float)\n",
    "        \n",
    "        if labels is not None:\n",
    "            self.labels = torch.tensor(labels[:min_len], dtype=torch.long)\n",
    "        self.has_labels = labels is not None\n",
    "        \n",
    "        print(f\"Dataset created with {min_len} samples (reduced from {len(input_ids)} and {len(topic_dist)})\")\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'topic_dist': self.topic_dist[idx]\n",
    "        }\n",
    "        \n",
    "        if self.has_multi_granularity:\n",
    "            item['topic_dist_25'] = self.topic_dist_25[idx]\n",
    "            \n",
    "        if self.has_labels:\n",
    "            item['labels'] = self.labels[idx]\n",
    "            \n",
    "        return item\n",
    "  \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch_input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        batch_attention = torch.stack([item['attention_mask'] for item in batch])\n",
    "        batch_topic = torch.stack([item['topic_dist'] for item in batch])\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': batch_input_ids,\n",
    "            'attention_mask': batch_attention,\n",
    "            'topic_dist': batch_topic\n",
    "        }\n",
    "        \n",
    "        # Check if multi-granularity topics are available\n",
    "        if 'topic_dist_25' in batch[0]:\n",
    "            batch_topic_25 = torch.stack([item['topic_dist_25'] for item in batch])\n",
    "            result['topic_dist_25'] = batch_topic_25\n",
    "        \n",
    "        # Check if labels are available\n",
    "        if 'labels' in batch[0]:\n",
    "            batch_labels = torch.stack([item['labels'] for item in batch])\n",
    "            result['labels'] = batch_labels\n",
    "            \n",
    "        return result\n",
    "\n",
    "def load_csv_data():\n",
    "    \"\"\"Load CSV files containing preprocessed text data with metadata\"\"\"\n",
    "    train_csv = pd.read_csv(os.path.join(BASE_PATH, 'train_data_balanced.csv'))\n",
    "    val_csv = pd.read_csv(os.path.join(BASE_PATH, 'val_data.csv'))\n",
    "    test_csv = pd.read_csv(os.path.join(BASE_PATH, 'test_data.csv'))\n",
    "    \n",
    "    print(f\"Loaded CSV data - Train: {len(train_csv)} samples, Val: {len(val_csv)} samples, Test: {len(test_csv)} samples\")\n",
    "    return train_csv, val_csv, test_csv\n",
    "\n",
    "# ===== Dataset & Data Loading =====\n",
    "def load_data(split, include_multi_granularity=True):\n",
    "    \"\"\"\n",
    "    Load preprocessed NumPy arrays for a given split with multi-granularity topics.\n",
    "    \n",
    "    Args:\n",
    "        split: Dataset split ('train', 'val', or 'test')\n",
    "        include_multi_granularity: Whether to include 25-topic distributions\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of arrays including multi-granularity topic distributions if requested\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load base data\n",
    "        input_ids = np.load(os.path.join(BASE_PATH, f'{split}_input_ids.npy'))\n",
    "        attention_mask = np.load(os.path.join(BASE_PATH, f'{split}_attention_mask.npy'))\n",
    "        topic_dist = np.load(os.path.join(BASE_PATH, f'{split}_lda_topics.npy'))\n",
    "        labels = np.load(os.path.join(BASE_PATH, f'{split}_labels.npy'))\n",
    "        \n",
    "        # Calculate minimum size with enhanced reporting for truncation\n",
    "        original_lengths = [len(input_ids), len(attention_mask), len(topic_dist), len(labels)]\n",
    "        min_size = min(original_lengths)\n",
    "        if min_size < max(original_lengths):\n",
    "            truncation_percentage = (max(original_lengths) - min_size) / max(original_lengths) * 100\n",
    "            print(f\"Warning: Truncating {split} data by {truncation_percentage:.2f}% due to length mismatch.\")\n",
    "            if truncation_percentage > 5:\n",
    "                print(f\"Critical Warning: Significant data loss (>5%) in {split} data due to truncation!\")\n",
    "        \n",
    "        print(f\"Truncating {split} data from {len(input_ids)}/{len(topic_dist)} to {min_size} samples\")\n",
    "        \n",
    "        # Truncate all arrays\n",
    "        input_ids = input_ids[:min_size]\n",
    "        attention_mask = attention_mask[:min_size]\n",
    "        topic_dist = topic_dist[:min_size]\n",
    "        labels = labels[:min_size]\n",
    "        \n",
    "        # Load and truncate 25-topic distributions if requested\n",
    "        if include_multi_granularity:\n",
    "            topic_dist_25 = np.load(os.path.join(BASE_PATH, f'{split}_lda_topics_25.npy'))\n",
    "            topic_dist_25 = topic_dist_25[:min_size]\n",
    "            # Validate LDA topic dimensions\n",
    "            expected_topics_15 = 15\n",
    "            expected_topics_25 = 25\n",
    "            if topic_dist.shape[1] != expected_topics_15 or topic_dist_25.shape[1] != expected_topics_25:\n",
    "                print(f\"Warning: LDA topic dimensions mismatch in {split} data! Expected {expected_topics_15} and {expected_topics_25}, \"\n",
    "                      f\"got {topic_dist.shape[1]} and {topic_dist_25.shape[1]}.\")\n",
    "            return input_ids, attention_mask, topic_dist, topic_dist_25, labels\n",
    "        else:\n",
    "            # Validate single LDA topic dimension\n",
    "            expected_topics_15 = 15\n",
    "            if topic_dist.shape[1] != expected_topics_15:\n",
    "                print(f\"Warning: LDA topic dimension mismatch in {split} data! Expected {expected_topics_15}, \"\n",
    "                      f\"got {topic_dist.shape[1]}.\")\n",
    "            return input_ids, attention_mask, topic_dist, labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {split}: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:26.749254Z",
     "iopub.status.busy": "2025-04-25T01:09:26.748626Z",
     "iopub.status.idle": "2025-04-25T01:09:26.782509Z",
     "shell.execute_reply": "2025-04-25T01:09:26.781733Z",
     "shell.execute_reply.started": "2025-04-25T01:09:26.749235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Model Components =====\n",
    "class MultiGranularityTopicModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced topic module that processes LDA distributions at multiple granularities.\n",
    "    Combines features from both 15-topic and 25-topic distributions for richer\n",
    "    symbolic representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, lda_topics=15, lda_topics_25=25, hidden_size=HIDDEN_SIZE, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # Make sure first parameter matches topic_dist dimension (15)\n",
    "        self.topic_encoder = nn.Sequential(\n",
    "            nn.Linear(lda_topics, hidden_size // 4),\n",
    "            nn.BatchNorm1d(hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size // 4, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2)\n",
    "        )\n",
    "        \n",
    "        # Additional encoder for 25-topic distributions\n",
    "        self.topic_encoder_25 = nn.Sequential(\n",
    "            nn.Linear(lda_topics_25, hidden_size // 4),\n",
    "            nn.BatchNorm1d(hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size // 4, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2)\n",
    "        )\n",
    "        \n",
    "        # Final projection to combine features\n",
    "        self.projection = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Xavier initialization for linear layers\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, gain=0.1)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, topic_dist, topic_dist_25=None):\n",
    "        # Process standard topics\n",
    "        topic_dist = torch.clamp(topic_dist, min=0.05)\n",
    "        topic_dist = topic_dist / torch.sum(topic_dist, dim=1, keepdim=True)\n",
    "        uniform_dist = torch.ones_like(topic_dist) / topic_dist.size(1)\n",
    "        topic_dist = 0.7 * topic_dist + 0.3 * uniform_dist\n",
    "        features_15 = self.topic_encoder(topic_dist)\n",
    "        \n",
    "        if topic_dist_25 is not None:\n",
    "            # Process 25-topic distributions\n",
    "            topic_dist_25 = torch.clamp(topic_dist_25, min=0.05)\n",
    "            topic_dist_25 = topic_dist_25 / torch.sum(topic_dist_25, dim=1, keepdim=True)\n",
    "            uniform_dist_25 = torch.ones_like(topic_dist_25) / topic_dist_25.size(1)\n",
    "            topic_dist_25 = 0.7 * topic_dist_25 + 0.3 * uniform_dist_25\n",
    "            features_25 = self.topic_encoder_25(topic_dist_25)\n",
    "            \n",
    "            # Concatenate features and project to final dimension\n",
    "            combined_features = torch.cat([features_15, features_25], dim=1)\n",
    "            return self.projection(combined_features)\n",
    "        else:\n",
    "            # Use zero padding instead of duplicating features\n",
    "            features_placeholder = torch.zeros_like(features_15)\n",
    "            combined_features = torch.cat([features_15, features_placeholder], dim=1)\n",
    "            return self.projection(combined_features)\n",
    "\n",
    "class UFENModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Extracts contextual text features using a pre-trained transformer.\n",
    "    Focuses solely on text processing without early metadata fusion.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=HIDDEN_SIZE, metadata_dim=NUM_TOPICS, use_distil=True):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        if hasattr(self.bert.encoder, 'albert_layer_groups'):\n",
    "            last_group_idx = len(self.bert.encoder.albert_layer_groups) - 1\n",
    "            for param in self.bert.encoder.albert_layer_groups[last_group_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "        elif hasattr(self.bert.encoder, 'layer'):\n",
    "            for param in self.bert.encoder.layer[-1].parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        self.projection = nn.Linear(self.bert.config.hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, metadata=None):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = bert_output.last_hidden_state\n",
    "        text_features = self.projection(text_features)\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        text_features = text_features * mask\n",
    "        return text_features\n",
    "\n",
    "class ECOSAMModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses multihead attention to focus on sentiment-specific signals.\n",
    "    Incorporates an additional learnable sentiment query and gating mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=HIDDEN_SIZE, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(hidden_size, hidden_size//2, bidirectional=True, batch_first=True)\n",
    "        self.context_attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n",
    "        self.attention_norm = nn.LayerNorm(hidden_size)\n",
    "        self.sentiment_query = nn.Parameter(torch.randn(1, 1, hidden_size) * 0.01)\n",
    "        self.sentiment_attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)\n",
    "        self.sent_dropout = nn.Dropout(dropout_rate)\n",
    "        self.sent_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.context_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, sequence_features):\n",
    "        # Removed clamping to preserve feature information; rely on LayerNorm for stability\n",
    "        lstm_out, _ = self.bilstm(sequence_features)\n",
    "        context_attn, _ = self.context_attention(lstm_out, lstm_out, lstm_out)\n",
    "        context_attn = self.context_dropout(context_attn)\n",
    "        context_enhanced = self.attention_norm(context_attn + lstm_out)\n",
    "        batch_size = sequence_features.size(0)\n",
    "        sent_query = self.sentiment_query.expand(batch_size, 1, -1)\n",
    "        sent_context, _ = self.sentiment_attention(sent_query, context_enhanced, context_enhanced)\n",
    "        sent_context = self.sent_dropout(sent_context).squeeze(1)\n",
    "        sent_gates = self.sent_gate(context_enhanced)\n",
    "        sent_gates = sent_gates / (sent_gates.sum(dim=1, keepdim=True) + 1e-9)\n",
    "        context_vector = (context_enhanced * sent_gates).sum(dim=1)\n",
    "        return context_vector\n",
    "\n",
    "class BidirectionalFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses neural text representations and symbolic topic features.\n",
    "    Uses learnable linear transformations and a scalar gate to dynamically balance information flow.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=HIDDEN_SIZE):\n",
    "        super().__init__()\n",
    "        self.text_to_topic = nn.Linear(hidden_size, hidden_size)\n",
    "        self.topic_to_text = nn.Linear(hidden_size, hidden_size)\n",
    "        self.gate = nn.Parameter(torch.tensor(0.5))\n",
    "    \n",
    "    def forward(self, text_features, topic_features):\n",
    "        text_enhanced = text_features + self.gate * self.topic_to_text(topic_features)\n",
    "        topic_enhanced = topic_features + (1 - self.gate) * self.text_to_topic(text_features)\n",
    "        return text_enhanced, topic_enhanced\n",
    "\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "class HybridSentimentModel(nn.Module, PyTorchModelHubMixin):\n",
    "    \"\"\"\n",
    "    Enhanced hybrid neuro-symbolic model that uses multi-granularity LDA topics.\n",
    "    Combines neural text features from transformers with symbolic features \n",
    "    from multiple LDA topic distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES, lda_topics=15, lda_topics_25=25, hidden_size=HIDDEN_SIZE, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.config = {\n",
    "            \"num_classes\": num_classes,\n",
    "            \"lda_topics\": lda_topics,\n",
    "            \"lda_topics_25\": lda_topics_25,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"bert_model_name\": BERT_MODEL_NAME\n",
    "        }\n",
    "        self.ufen = UFENModule(hidden_size=hidden_size, metadata_dim=lda_topics, use_distil=True)\n",
    "        self.topic_module = MultiGranularityTopicModule(lda_topics, lda_topics_25, hidden_size, dropout_rate=dropout_rate)\n",
    "        self.ecosam = ECOSAMModule(hidden_size=hidden_size, dropout_rate=dropout_rate)\n",
    "        self.bidir_fusion = BidirectionalFusion(hidden_size=hidden_size)\n",
    "        self.drop_path = StochasticDepth(p=dropout_rate, mode='batch')\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            self.drop_path,\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, topic_dist, topic_dist_25=None):\n",
    "        # Process symbolic features from LDA topics using multi-granularity module\n",
    "        topic_features = self.topic_module(topic_dist, topic_dist_25)\n",
    "        \n",
    "        # Extract text features from transformer\n",
    "        sequence_features = self.ufen(input_ids, attention_mask)\n",
    "        \n",
    "        # Apply context-aware attention to get sentiment-focused representation\n",
    "        context_vector = self.ecosam(sequence_features)\n",
    "        \n",
    "        # Use the sentiment-focused context vector for fusion\n",
    "        text_representation = context_vector\n",
    "        \n",
    "        # Fuse representations bidirectionally\n",
    "        enhanced_text, enhanced_topic = self.bidir_fusion(text_representation, topic_features)\n",
    "        fused_features = enhanced_text + enhanced_topic\n",
    "        \n",
    "        # Classification layer outputs final sentiment logits\n",
    "        logits = self.classifier(fused_features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:26.783613Z",
     "iopub.status.busy": "2025-04-25T01:09:26.783356Z",
     "iopub.status.idle": "2025-04-25T01:09:26.805624Z",
     "shell.execute_reply": "2025-04-25T01:09:26.805088Z",
     "shell.execute_reply.started": "2025-04-25T01:09:26.783591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Evaluation Functions =====\n",
    "def calculate_metrics(y_true, y_pred, class_names=None):\n",
    "    if torch.is_tensor(y_true):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    if torch.is_tensor(y_pred):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"confusion_matrix\": cm}\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=None):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(cm.shape[0])]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics_history(metrics_history):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(metrics_history['train_acc'], label='Train')\n",
    "    plt.plot(metrics_history['val_acc'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(metrics_history['train_loss'], label='Train')\n",
    "    plt.plot(metrics_history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(metrics_history['train_f1'], label='Train')\n",
    "    plt.plot(metrics_history['val_f1'], label='Validation')\n",
    "    plt.title('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(metrics_history['val_precision'], label='Precision')\n",
    "    plt.plot(metrics_history['val_recall'], label='Recall')\n",
    "    plt.title('Precision and Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:26.806655Z",
     "iopub.status.busy": "2025-04-25T01:09:26.806384Z",
     "iopub.status.idle": "2025-04-25T01:09:26.826247Z",
     "shell.execute_reply": "2025-04-25T01:09:26.825705Z",
     "shell.execute_reply.started": "2025-04-25T01:09:26.806638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def analyze_csv_data(csv_data, topic_model, vectorizer):\n",
    "    \"\"\"\n",
    "    Analyze text data and topic distributions from CSV files\n",
    "    to gain additional insights about the sentiment model.\n",
    "    \"\"\"\n",
    "    # Extract text samples with their topic distributions\n",
    "    text_samples = csv_data['text'].values[:5]\n",
    "    \n",
    "    # Use the LDA model to regenerate topic distributions for verification\n",
    "    text_dtm = vectorizer.transform(text_samples)\n",
    "    topic_distributions = topic_model.transform(text_dtm)\n",
    "    \n",
    "    # Compare topics across different granularities\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get top words for each topic\n",
    "    for topic_idx, topic in enumerate(topic_model.components_[:3]):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        print(f\"Topic #{topic_idx}: {', '.join(top_words)}\")\n",
    "    \n",
    "    return {\n",
    "        \"sample_topics\": topic_distributions,\n",
    "        \"topic_words\": {i: [feature_names[idx] for idx in topic.argsort()[:-11:-1]] \n",
    "                       for i, topic in enumerate(topic_model.components_[:5])}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:26.827251Z",
     "iopub.status.busy": "2025-04-25T01:09:26.827012Z",
     "iopub.status.idle": "2025-04-25T01:09:26.859497Z",
     "shell.execute_reply": "2025-04-25T01:09:26.858717Z",
     "shell.execute_reply.started": "2025-04-25T01:09:26.827234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Enhanced Training Functions with Mixed Precision and Early Stopping =====\n",
    "def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, lr=LEARNING_RATE):\n",
    "    # Set up mixed precision training\n",
    "    scaler = GradScaler(enabled=USE_MIXED_PRECISION) if torch.cuda.is_available() else None\n",
    "    \n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        adjusted_lr = lr * (num_gpus ** 0.5)\n",
    "        print(f\"Adjusting learning rate for {num_gpus}-GPU training: {adjusted_lr:.6f}\")\n",
    "        lr = adjusted_lr\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler with weight decay from best_config\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=best_config[\"weight_decay\"])\n",
    "    \n",
    "    # Use OneCycleLR for better convergence\n",
    "    steps_per_epoch = len(train_loader) // GRADIENT_ACCUMULATION_STEPS\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=lr,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,\n",
    "        final_div_factor=10000.0,\n",
    "    )\n",
    "    \n",
    "    # Validate class weights length\n",
    "    if len(class_weights) != NUM_CLASSES:\n",
    "        print(f\"Warning: class_weights length ({len(class_weights)}) doesn't match NUM_CLASSES ({NUM_CLASSES}).\")\n",
    "        # Fallback to uniform weights if mismatch\n",
    "        class_weights_tensor = torch.ones(NUM_CLASSES, dtype=torch.float).to(device)\n",
    "    else:\n",
    "        class_weights_tensor = torch.tensor([class_weights[i] for i in range(NUM_CLASSES)], dtype=torch.float).to(device)\n",
    "    weighted_loss = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    try:\n",
    "        class_names = sentiment_encoder.classes_\n",
    "    except:\n",
    "        class_names = [str(i) for i in range(NUM_CLASSES)]\n",
    "    \n",
    "    metrics_history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_precision': [],\n",
    "        'val_recall': [], 'val_f1': []\n",
    "    }\n",
    "    \n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training Progress\", unit=\"epoch\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False, unit=\"batch\")\n",
    "        optimizer.zero_grad()  # Zero gradients at the beginning of epoch for gradient accumulation\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            topic_dist = batch['topic_dist'].to(device)\n",
    "            topic_dist_25 = batch['topic_dist_25'].to(device) if 'topic_dist_25' in batch else None\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            if USE_MIXED_PRECISION and torch.cuda.is_available():\n",
    "                with autocast('cuda'):\n",
    "                    outputs = model(input_ids, attention_mask, topic_dist, topic_dist_25)\n",
    "                    loss = weighted_loss(outputs, labels)\n",
    "                    # Scale loss for gradient accumulation\n",
    "                    loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Update weights if we've accumulated enough gradients\n",
    "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                # Standard precision training\n",
    "                outputs = model(input_ids, attention_mask, topic_dist, topic_dist_25)\n",
    "                loss = weighted_loss(outputs, labels)\n",
    "                loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "                loss.backward()\n",
    "                \n",
    "                if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "            # Track metrics\n",
    "            batch_loss = loss.item() * GRADIENT_ACCUMULATION_STEPS  # Unadjust loss for reporting\n",
    "            total_loss += batch_loss * labels.size(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                total_correct += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                all_train_preds.extend(preds.cpu().numpy())\n",
    "                all_train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            batch_acc = (preds == labels).sum().item() / labels.size(0)\n",
    "            train_pbar.set_postfix(loss=f\"{batch_loss:.4f}\", acc=f\"{batch_acc:.4f}\")\n",
    "        \n",
    "        # Clean up any remaining gradients\n",
    "        if len(train_loader) % GRADIENT_ACCUMULATION_STEPS != 0:\n",
    "            scaler.unscale_(optimizer) if scaler else None\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n",
    "            scaler.step(optimizer) if scaler else optimizer.step()\n",
    "            scaler.update() if scaler else None\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = total_loss / total_samples\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_metrics = calculate_metrics(np.array(all_train_labels), np.array(all_train_preds))\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics_history['train_loss'].append(avg_loss)\n",
    "        metrics_history['train_acc'].append(train_acc)\n",
    "        metrics_history['train_f1'].append(train_metrics['f1'])\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", leave=False, unit=\"batch\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                topic_dist = batch['topic_dist'].to(device)\n",
    "                topic_dist_25 = batch['topic_dist_25'].to(device) if 'topic_dist_25' in batch else None\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                # Use mixed precision for validation as well\n",
    "                if USE_MIXED_PRECISION and torch.cuda.is_available():\n",
    "                    with autocast('cuda'):\n",
    "                        outputs = model(input_ids, attention_mask, topic_dist, topic_dist_25)\n",
    "                        loss = weighted_loss(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(input_ids, attention_mask, topic_dist, topic_dist_25)\n",
    "                    loss = weighted_loss(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * labels.size(0)\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_samples += labels.size(0)\n",
    "                all_val_preds.extend(preds.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                batch_acc = (preds == labels).sum().item() / labels.size(0)\n",
    "                val_pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{batch_acc:.4f}\")\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        avg_val_loss = val_loss / val_samples\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_metrics = calculate_metrics(np.array(all_val_labels), np.array(all_val_preds))\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics_history['val_loss'].append(avg_val_loss)\n",
    "        metrics_history['val_acc'].append(val_acc)\n",
    "        metrics_history['val_precision'].append(val_metrics['precision'])\n",
    "        metrics_history['val_recall'].append(val_metrics['recall'])\n",
    "        metrics_history['val_f1'].append(val_metrics['f1'])\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_pbar.set_postfix(\n",
    "            train_loss=f\"{avg_loss:.4f}\", \n",
    "            train_f1=f\"{train_metrics['f1']:.4f}\", \n",
    "            val_f1=f\"{val_metrics['f1']:.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"  Train - Loss: {avg_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_metrics['f1']:.4f}\")\n",
    "        print(f\"  Val   - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}, Precision: {val_metrics['precision']:.4f}, \" \n",
    "              f\"Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "        \n",
    "        # Check if this is the best model with combined metric for early stopping\n",
    "        current_metric = 0.7 * val_metrics['f1'] + 0.3 * (val_metrics['precision'] + val_metrics['recall'])/2\n",
    "        if current_metric > best_f1:\n",
    "            best_f1 = current_metric\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            \n",
    "            # Save the model\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                torch.save(model.module.state_dict(), MODEL_SAVE_PATH)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            \n",
    "            print(f\"New best model saved with combined metric (F1-based): {best_f1:.4f}\")\n",
    "            \n",
    "            # Plot confusion matrix for best model\n",
    "            cm = val_metrics['confusion_matrix']\n",
    "            plot_confusion_matrix(cm, class_names)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Combined metric did not improve. Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "            torch.cuda.empty_cache()\n",
    "        # Clean up GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Training completed. Best combined metric (F1-based): {best_f1:.4f}\")\n",
    "    plot_metrics_history(metrics_history)\n",
    "    torch.cuda.empty_cache()\n",
    "    # Load the best model for return\n",
    "    best_model_state = torch.load(MODEL_SAVE_PATH)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(best_model_state)\n",
    "    else:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names=None):\n",
    "    model.eval()\n",
    "    if class_names is None:\n",
    "        try:\n",
    "            class_names = sentiment_encoder.classes_\n",
    "        except:\n",
    "            class_names = [str(i) for i in range(NUM_CLASSES)]\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Add progress bar for test evaluation\n",
    "    test_pbar = tqdm(test_loader, desc=\"Evaluating on Test Set\", unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            topic_dist = batch['topic_dist'].to(device)\n",
    "            topic_dist_25 = batch['topic_dist_25'].to(device) if 'topic_dist_25' in batch else None\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Use mixed precision for evaluation\n",
    "            if USE_MIXED_PRECISION and torch.cuda.is_available():\n",
    "                with autocast('cuda'):\n",
    "                    outputs = model(input_ids, attention_mask, topic_dist, topic_dist_25)\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask, topic_dist, topic_dist_25)\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update test progress bar\n",
    "            batch_acc = (preds == labels).sum().item() / labels.size(0)\n",
    "            test_pbar.set_postfix(acc=f\"{batch_acc:.4f}\")\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    metrics = calculate_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    \n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score:  {metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = metrics['confusion_matrix']\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "    \n",
    "    # Per-class metrics for detailed analysis\n",
    "    classes = np.unique(all_labels)\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    for cls in classes:\n",
    "        cls_indices = np.array(all_labels) == cls\n",
    "        cls_preds = np.array(all_preds)[cls_indices]\n",
    "        cls_true = np.array(all_labels)[cls_indices]\n",
    "        cls_acc = accuracy_score(cls_true, cls_preds)\n",
    "        cls_prec = precision_score(cls_true, cls_preds, average='macro', zero_division=0)\n",
    "        cls_rec = recall_score(cls_true, cls_preds, average='macro', zero_division=0)\n",
    "        cls_f1 = f1_score(cls_true, cls_preds, average='macro', zero_division=0)\n",
    "        \n",
    "        print(f\"Class {class_names[cls]}:\")\n",
    "        print(f\"  Accuracy: {cls_acc:.4f}, Precision: {cls_prec:.4f}, Recall: {cls_rec:.4f}, F1: {cls_f1:.4f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T01:09:26.860565Z",
     "iopub.status.busy": "2025-04-25T01:09:26.860288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Main Execution =====\n",
    "if __name__ == \"__main__\":\n",
    "    # Fix for \"can only test a child process\" error\n",
    "    import torch.multiprocessing as mp\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    # Load CSV data for additional analysis\n",
    "    train_csv, val_csv, test_csv = load_csv_data()\n",
    "    \n",
    "    print(\"Analyzing topic distributions in CSV data...\")\n",
    "    topic_columns = [col for col in train_csv.columns if col.startswith('topic_')]\n",
    "    if topic_columns:\n",
    "        print(f\"Found {len(topic_columns)} topic columns in CSV data\")\n",
    "    \n",
    "    # Use pre-determined hyperparameters instead of tuning\n",
    "    best_config = {\n",
    "        'lr': 3e-05, \n",
    "        'weight_decay': 0.01, \n",
    "        'hidden_size': 768, \n",
    "        'dropout_rate': 0.1, \n",
    "        'batch_size': 16\n",
    "    }\n",
    "    \n",
    "    print(\"\\n==== Using Pre-determined Hyperparameters ====\")\n",
    "    print(f\"Config: {best_config}\")\n",
    "    \n",
    "    # Load data for training\n",
    "    train_input_ids, train_attention, train_topic, train_topic_25, train_labels = load_data('train', include_multi_granularity=True)\n",
    "    val_input_ids, val_attention, val_topic, val_topic_25, val_labels = load_data('val', include_multi_granularity=True)\n",
    "    test_input_ids, test_attention, test_topic, test_topic_25, test_labels = load_data('test', include_multi_granularity=True)\n",
    "    \n",
    "    # Print shape information for verification\n",
    "    print(f\"Train topics shape: {train_topic.shape}, Train topics 25 shape: {train_topic_25.shape}\")\n",
    "    \n",
    "    # Create datasets with multi-granularity topics\n",
    "    train_dataset = SentimentDataset(train_input_ids, train_attention, train_topic, train_topic_25, train_labels)\n",
    "    val_dataset = SentimentDataset(val_input_ids, val_attention, val_topic, val_topic_25, val_labels)\n",
    "    test_dataset = SentimentDataset(test_input_ids, test_attention, test_topic, test_topic_25, test_labels)\n",
    "    \n",
    "    # Batch size from config\n",
    "    effective_batch_size = best_config[\"batch_size\"]\n",
    "    if torch.cuda.device_count() >= 2:\n",
    "        effective_batch_size *= torch.cuda.device_count()\n",
    "        print(f\"Using effective batch size of {effective_batch_size} with {torch.cuda.device_count()} GPUs\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=effective_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=SentimentDataset.collate_fn,\n",
    "        num_workers=0,     \n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=effective_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=SentimentDataset.collate_fn,\n",
    "        num_workers=0,      \n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=effective_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=SentimentDataset.collate_fn,\n",
    "        num_workers=0,    \n",
    "        pin_memory=True,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "\n",
    "    # Initialize the model with specified hyperparameters\n",
    "    model = HybridSentimentModel(\n",
    "        num_classes=NUM_CLASSES, \n",
    "        lda_topics=train_topic.shape[1], \n",
    "        lda_topics_25=train_topic_25.shape[1], \n",
    "        hidden_size=best_config[\"hidden_size\"]\n",
    "    )\n",
    "    \n",
    "    # Update dropout rates\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.p = best_config[\"dropout_rate\"]\n",
    "    \n",
    "    # Multi-GPU setup if available\n",
    "    if torch.cuda.device_count() >= 2:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "        torch.cuda.set_device(0) \n",
    "        model = model.to(torch.device('cuda:0'))\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    else:\n",
    "        print(f\"Using single GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Train the model with specified hyperparameters\n",
    "    trained_model = train_model(model, train_loader, val_loader, \n",
    "                               num_epochs=NUM_EPOCHS, \n",
    "                               lr=best_config[\"lr\"])\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    try:\n",
    "        class_names = sentiment_encoder.classes_\n",
    "    except:\n",
    "        class_names = [str(i) for i in range(NUM_CLASSES)]\n",
    "        \n",
    "    test_metrics = evaluate_model(trained_model, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "model = HybridSentimentModel(\n",
    "    num_classes=NUM_CLASSES,  \n",
    "    lda_topics=train_topic.shape[1], \n",
    "    lda_topics_25=train_topic_25.shape[1],  \n",
    "    hidden_size=768,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Push the model to Hugging Face Hub\n",
    "model.push_to_hub(\"aiguy68/neurosam-model\")\n",
    "\n",
    "# Also push the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "tokenizer.push_to_hub(\"aiguy68/neurosam-model\")\n",
    "\n",
    "# Upload LDA components (if needed)\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "# Save locally first\n",
    "import pickle\n",
    "with open(\"count_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(count_vectorizer, f)\n",
    "with open(\"lda_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lda_model, f)\n",
    "\n",
    "# Upload to Hub\n",
    "upload_file(\n",
    "    path_or_fileobj=\"count_vectorizer.pkl\",\n",
    "    path_in_repo=\"count_vectorizer.pkl\",\n",
    "    repo_id=\"aiguy68/neurosam-model\"\n",
    ")\n",
    "upload_file(\n",
    "    path_or_fileobj=\"lda_model.pkl\", \n",
    "    path_in_repo=\"lda_model.pkl\",\n",
    "    repo_id=\"aiguy68/neurosam-model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7205379,
     "sourceId": 11494161,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
